{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yQHfXgNJ0wqj"},"outputs":[],"source":["from modulefinder import packagePathMap\n","from unittest import result\n","from click import pass_obj\n","from sentence_transformers import SentenceTransformer\n","from transformers import AutoTokenizer, AutoModel\n","from rank_bm25 import BM25Okapi\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from simcse import SimCSE\n","import json\n","import jsonlines\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZOzOGtD0wql","outputId":"f02fe30e-c774-444c-b2aa-3c0c091628b2"},"outputs":[{"name":"stderr","output_type":"stream","text":["03/15/2022 16:51:15 - INFO - sentence_transformers.SentenceTransformer -   Load pretrained SentenceTransformer: nli-roberta-base-v2\n","03/15/2022 16:51:46 - INFO - sentence_transformers.SentenceTransformer -   Use pytorch device: cuda\n"]}],"source":["model_sent_bert_nli = SentenceTransformer('nli-roberta-base-v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQv6rtnQ0wqm","outputId":"a86c7882-5ac6-4ec4-cc4e-24e7e5c67ef0"},"outputs":[{"name":"stderr","output_type":"stream","text":["03/15/2022 16:58:43 - INFO - sentence_transformers.SentenceTransformer -   Load pretrained SentenceTransformer: paraphrase-TinyBERT-L6-v2\n","03/15/2022 16:59:01 - INFO - sentence_transformers.SentenceTransformer -   Use pytorch device: cuda\n"]}],"source":["model_sent_bert_pp = SentenceTransformer('paraphrase-TinyBERT-L6-v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WaXibwHh0wqm","outputId":"e0ae14f0-bb47-4100-d01e-753cdb84876e"},"outputs":[{"name":"stderr","output_type":"stream","text":["03/15/2022 17:17:49 - INFO - simcse.tool -   Use `cls_before_pooler` for unsupervised models. If you want to use other pooling policy, specify `pooler` argument.\n"]}],"source":["model_unsimcse = SimCSE(\"princeton-nlp/unsup-simcse-bert-base-uncased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvH6X2rc0wqn"},"outputs":[],"source":["model_susimcse = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tGbameP0wqn"},"outputs":[],"source":["tokenizer_specter = AutoTokenizer.from_pretrained('allenai/specter')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szvIxt5H0wqn"},"outputs":[],"source":["model_specter = AutoModel.from_pretrained('allenai/specter')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJkyzVu00wqn"},"outputs":[],"source":["tokenizer_scibert_uncased = AutoTokenizer.from_pretrained(\n","    'allenai/scibert_scivocab_uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKu4reut0wqo","outputId":"af5184bb-d19a-4e6d-e662-4b6c6aece1ad"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model_scibert_uncased = AutoModel.from_pretrained(\n","    'allenai/scibert_scivocab_uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qi6oynZC0wqo"},"outputs":[],"source":["tokenizer_scibert_cased = AutoTokenizer.from_pretrained(\n","    'allenai/scibert_scivocab_cased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-05GhCp0wqo","outputId":"8f086582-b762-4e70-e43f-c10cc0537871"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model_scibert_cased = AutoModel.from_pretrained(\n","    'allenai/scibert_scivocab_cased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85LoAcKC0wqp"},"outputs":[],"source":["def get_bert_nli_embedding(sentence):\n","    return model_sent_bert_nli.encode(sentence)\n","\n","\n","def get_bert_pp_embedding(sentence):\n","    return model_sent_bert_pp.encode(sentence)\n","\n","\n","def get_specter_embedding(sentence):\n","    # TODO: check if max_length is required\n","    inputs = tokenizer_specter(\n","        sentence, padding=True, truncation=True, return_tensors=\"pt\", max_length=65536)\n","    return model_specter(**inputs).last_hidden_state[:, 0, :]\n","\n","\n","def get_scibert_uncased_embedding(sentence):\n","    inputs = tokenizer_scibert_uncased(\n","        sentence, padding=True, truncation=True, return_tensors=\"pt\", max_length=65536)\n","    return model_scibert_uncased(**inputs).last_hidden_state[:, 0, :]\n","\n","\n","def get_scibert_cased_embedding(sentence):\n","    inputs = tokenizer_scibert_cased(\n","        sentence, padding=True, truncation=True, return_tensors=\"pt\", max_length=65536)\n","    return model_scibert_cased(**inputs).last_hidden_state[:, 0, :]\n","\n","\n","def get_unsimcse_embedding(sentence):\n","    return model_unsimcse.encode(sentence)\n","\n","\n","def get_susimcse_embedding(sentence):\n","    return model_susimcse.encode(sentence)\n","\n","def tfidf_fit_corpus(corpus):\n","    tfidf = TfidfVectorizer().fit([corpus])\n","    return tfidf\n","\n","def get_tfidf_embedding(tfidf, query):\n","    return tfidf.transform([query]).toarray()[0]\n","\n","\n","def get_bm25_score(corpus, query):\n","    tokenized_corpus = [doc.split(\" \") for doc in corpus]\n","    tokenized_query = query.split(\" \")\n","    bm25 = BM25Okapi(tokenized_corpus)\n","    return bm25.get_scores(tokenized_query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5ibVI7P0wqp","outputId":"685243a0-54a9-4bc0-f861-cbd6309c8943"},"outputs":[{"name":"stdout","output_type":"stream","text":["4207\n","16\n","17\n","17\n"]}],"source":["\n","class Paper:\n","    def __init__(self, paper_id, metadata, title, abstract, pred_labels_truncated, pred_labels):\n","        self.paper_id = paper_id\n","        self.metadata = metadata\n","        self.title = title\n","        self.abstract = abstract\n","        self.pred_labels_truncated = pred_labels_truncated\n","        self.pred_labels = pred_labels\n","\n","papers = dict()\n","with jsonlines.open('../data/abstracts-csfcube-preds.jsonl') as reader:\n","    for obj in reader:\n","        papers[obj['paper_id']] = Paper(obj['paper_id'], obj['metadata'], obj['title'], obj['abstract'], obj['pred_labels_truncated'], obj['pred_labels'])\n","\n","f = open('../data/test-pid2anns-csfcube-background.json')\n","q_background = json.load(f)\n","q_background_ids = list(q_background.keys())\n","\n","f = open('../data/test-pid2anns-csfcube-method.json')\n","q_method = json.load(f)\n","q_method_ids = list(q_method.keys())\n","\n","f = open('../data/test-pid2anns-csfcube-result.json')\n","q_result = json.load(f)\n","q_result_ids = list(q_result.keys())\n","\n","all_ids = list(papers.keys())\n","\n","# candidate_ids = list(set(all_ids) - set(q_background_ids) - set(q_method_ids) - set(q_result_ids))\n","\n","print(len(all_ids))\n","print(len(q_background_ids))\n","print(len(q_method_ids))\n","print(len(q_result_ids))\n","# print(len(candidate_ids))\n","\n","def facet_sentences(pid, facet):\n","    labels = {'background': ['background_label', 'objective_label'], 'method': ['method_label'], 'result': ['result_label']}\n","    paper = papers[pid]\n","    s = list()\n","    for i in range(len(paper.pred_labels)):\n","        if paper.pred_labels[i] in labels[facet]:\n","            s.append(paper.abstract[i])\n","    return s\n","\n","def merge_list_to_str(l):\n","    return \" \".join(l)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MT9jSDdw0wqp"},"outputs":[],"source":["corpus_list = list()\n","for pid in papers:\n","    doc = papers[pid].abstract\n","    corpus_list = corpus_list + doc\n","corpus = merge_list_to_str(corpus_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-5Ohc-m0wqq"},"outputs":[],"source":["tfidf = tfidf_fit_corpus(corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53IfnFZ_0wqq","outputId":"e632f530-c9fe-470d-b191-bc35482ddb31"},"outputs":[{"name":"stdout","output_type":"stream","text":["50\n","100\n","150\n","200\n","250\n","300\n","350\n","400\n","450\n","500\n","550\n","600\n","650\n","700\n","750\n","800\n","850\n","900\n","950\n","1000\n","1050\n","1100\n","1150\n","1200\n","1250\n","1300\n","1350\n","1400\n","1450\n","1500\n","1550\n","1600\n","1650\n","1700\n","1750\n","1800\n","1850\n","1900\n","1950\n","2000\n","2050\n","2100\n","2150\n","2200\n","2250\n","2300\n","2350\n","2400\n","2450\n","2500\n","2550\n","2600\n","2650\n","2700\n","2750\n","2800\n","2850\n","2900\n","2950\n","3000\n","3050\n","3100\n","3150\n","3200\n","3250\n","3300\n","3350\n","3400\n","3450\n","3500\n","3550\n","3600\n","3650\n","3700\n","3750\n","3800\n","3850\n","3900\n","3950\n","4000\n","4050\n","4100\n","4150\n","4200\n","len(err_pids) = 0\n","[]\n"]}],"source":["# tf_idf all\n","d = dict()\n","err_pids = list()\n","i = 0\n","for pid in papers:\n","    i += 1\n","    if i % 50 == 0:\n","        print(i)\n","    try:\n","        doc = papers[pid].abstract\n","        if len(doc) == 0:\n","            d[pid] = list()\n","            continue\n","        bg_tfidf_array = np.array(get_tfidf_embedding(tfidf, merge_list_to_str(doc))).tolist()\n","        d[pid] = bg_tfidf_array\n","    except:\n","        err_pids.append(pid)\n","print(f'len(err_pids) = {len(err_pids)}')\n","print(err_pids)\n","with open(\"embeddings/tf_idf/all.json\", \"w\") as outfile:\n","    json.dump(d, outfile)\n","d = json.load(open('embeddings/tf_idf/all.json'),\n","              parse_float=lambda x: round(float(x), 6))\n","with open('embeddings/tf_idf/all.json', \"w\") as outfile:\n","    json.dump(d, outfile)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gEovgPDV0wqq","outputId":"d38e14d1-e1bf-4024-9be3-334b54beb8b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["len(err_pids), background = 0\n","[]\n","len(err_pids), method = 0\n","[]\n","len(err_pids), result = 0\n","[]\n"]}],"source":["# tf_idf queries\n","for i, ids in enumerate([q_background_ids, q_method_ids, q_result_ids]):\n","    d = dict()\n","    err_pids = list()\n","    s = ''\n","    if i == 0:\n","        s = 'background'\n","    elif i == 1:\n","        s = 'method'\n","    elif i == 2:\n","        s = 'result'\n","    for pid in ids:\n","        try:\n","            doc = facet_sentences(pid, s)\n","            if len(doc) == 0:\n","                d[pid] = list()\n","                continue\n","            tfidf_array = np.array(get_tfidf_embedding(tfidf, merge_list_to_str(doc))).tolist()\n","            d[pid] = tfidf_array\n","        except:\n","            err_pids.append(pid)\n","    print(f'len(err_pids), {s} = {len(err_pids)}')\n","    print(err_pids)\n","    with open(f'embeddings/tf_idf/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n","    d = json.load(open(f'embeddings/tf_idf/{s}.json'),\n","                parse_float=lambda x: round(float(x), 6))\n","    with open(f'embeddings/tf_idf/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHbj6ThB0wqq"},"outputs":[],"source":["# bert_nli all\n","d = dict()\n","err_pids = list()\n","i = 0\n","for pid in papers:\n","    # i += 1\n","    # if i % 50 == 0:\n","    #     print(i)\n","    try:\n","        doc = papers[pid].abstract\n","        if len(doc) == 0:\n","            d[pid] = list()\n","            continue\n","        bert_nli_array = np.array(get_bert_nli_embedding(merge_list_to_str(doc))).tolist()\n","        d[pid] = bert_nli_array\n","    except:\n","        err_pids.append(pid)\n","print(f'len(err_pids) = {len(err_pids)}')\n","print(err_pids)\n","with open(\"embeddings/bert_nli/all.json\", \"w\") as outfile:\n","    json.dump(d, outfile)\n","d = json.load(open('embeddings/bert_nli/all.json'),\n","              parse_float=lambda x: round(float(x), 6))\n","with open('embeddings/bert_nli/all.json', \"w\") as outfile:\n","    json.dump(d, outfile)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQBU6ll00wqq"},"outputs":[],"source":["# bert_nli queries\n","for i, ids in enumerate([q_background_ids, q_method_ids, q_result_ids]):\n","    d = dict()\n","    err_pids = list()\n","    s = ''\n","    if i == 0:\n","        s = 'background'\n","    elif i == 1:\n","        s = 'method'\n","    elif i == 2:\n","        s = 'result'\n","    for pid in ids:\n","        try:\n","            doc = facet_sentences(pid, s)\n","            if len(doc) == 0:\n","                d[pid] = list()\n","                continue\n","            bert_nli_array = np.array(get_bert_nli_embedding(merge_list_to_str(doc))).tolist()\n","            d[pid] = bert_nli_array\n","        except:\n","            err_pids.append(pid)\n","    print(f'len(err_pids), {s} = {len(err_pids)}')\n","    print(err_pids)\n","    with open(f'embeddings/bert_nli/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n","    d = json.load(open(f'embeddings/bert_nli/{s}.json'),\n","                parse_float=lambda x: round(float(x), 6))\n","    with open(f'embeddings/bert_nli/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCKUaB-A0wqq"},"outputs":[],"source":["# bert_pp all\n","d = dict()\n","err_pids = list()\n","i = 0\n","for pid in papers:\n","    # i += 1\n","    # if i % 50 == 0:\n","    #     print(i)\n","    try:\n","        doc = papers[pid].abstract\n","        if len(doc) == 0:\n","            d[pid] = list()\n","            continue\n","        bert_pp_array = np.array(get_bert_pp_embedding(merge_list_to_str(doc))).tolist()\n","        d[pid] = bert_pp_array\n","    except:\n","        err_pids.append(pid)\n","print(f'len(err_pids) = {len(err_pids)}')\n","print(err_pids)\n","with open(\"embeddings/bert_pp/all.json\", \"w\") as outfile:\n","    json.dump(d, outfile)\n","d = json.load(open('embeddings/bert_pp/all.json'),\n","              parse_float=lambda x: round(float(x), 6))\n","with open('embeddings/bert_pp/all.json', \"w\") as outfile:\n","    json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJ6GkV5m0wqr"},"outputs":[],"source":["# bert_pp queries\n","for i, ids in enumerate([q_background_ids, q_method_ids, q_result_ids]):\n","    d = dict()\n","    err_pids = list()\n","    s = ''\n","    if i == 0:\n","        s = 'background'\n","    elif i == 1:\n","        s = 'method'\n","    elif i == 2:\n","        s = 'result'\n","    for pid in ids:\n","        try:\n","            doc = facet_sentences(pid, s)\n","            if len(doc) == 0:\n","                d[pid] = list()\n","                continue\n","            bert_pp_array = np.array(get_bert_pp_embedding(merge_list_to_str(doc))).tolist()\n","            d[pid] = bert_pp_array\n","        except:\n","            err_pids.append(pid)\n","    print(f'len(err_pids), {s} = {len(err_pids)}')\n","    print(err_pids)\n","    with open(f'embeddings/bert_pp/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n","    d = json.load(open(f'embeddings/bert_pp/{s}.json'),\n","                parse_float=lambda x: round(float(x), 6))\n","    with open(f'embeddings/bert_pp/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZ7l58CH0wqr"},"outputs":[],"source":["# unsimcse all\n","d = dict()\n","err_pids = list()\n","i = 0\n","for pid in papers:\n","    # if i == 1:\n","    #     break\n","    # i += 1\n","    # if i % 50 == 0:\n","    #     print(i)\n","    try:\n","        doc = papers[pid].abstract\n","        if len(doc) == 0:\n","            d[pid] = list()\n","            continue\n","        unsimcse_array = get_unsimcse_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","        # print(len(unsimcse_array))\n","        d[pid] = unsimcse_array\n","    except:\n","        err_pids.append(pid)\n","print(f'len(err_pids) = {len(err_pids)}')\n","print(err_pids)\n","with open(\"embeddings/unsimcse/all.json\", \"w\") as outfile:\n","    json.dump(d, outfile)\n","d = json.load(open('embeddings/unsimcse/all.json'),\n","              parse_float=lambda x: round(float(x), 6))\n","with open('embeddings/unsimcse/all.json', \"w\") as outfile:\n","    json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C380UOjp0wqr"},"outputs":[],"source":["# unsimcse queries\n","for i, ids in enumerate([q_background_ids, q_method_ids, q_result_ids]):\n","    d = dict()\n","    err_pids = list()\n","    s = ''\n","    if i == 0:\n","        s = 'background'\n","    elif i == 1:\n","        s = 'method'\n","    elif i == 2:\n","        s = 'result'\n","    for pid in ids:\n","        try:\n","            doc = facet_sentences(pid, s)\n","            if len(doc) == 0:\n","                d[pid] = list()\n","                continue\n","            unsimcse_array = get_unsimcse_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","            d[pid] = unsimcse_array\n","        except:\n","            err_pids.append(pid)\n","    print(f'len(err_pids), {s} = {len(err_pids)}')\n","    print(err_pids)\n","    with open(f'embeddings/unsimcse/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n","    d = json.load(open(f'embeddings/unsimcse/{s}.json'),\n","                parse_float=lambda x: round(float(x), 6))\n","    with open(f'embeddings/unsimcse/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tVOiMOQ0wqr"},"outputs":[],"source":["# susimcse all\n","d = dict()\n","err_pids = list()\n","i = 0\n","for pid in papers:\n","    # i += 1\n","    # if i % 50 == 0:\n","    #     print(i)\n","    try:\n","        doc = papers[pid].abstract\n","        if len(doc) == 0:\n","            d[pid] = list()\n","            continue\n","        susimcse_array = get_susimcse_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","        d[pid] = susimcse_array\n","    except:\n","        err_pids.append(pid)\n","print(f'len(err_pids) = {len(err_pids)}')\n","print(err_pids)\n","with open(\"embeddings/susimcse/all.json\", \"w\") as outfile:\n","    json.dump(d, outfile)\n","d = json.load(open('embeddings/susimcse/all.json'),\n","              parse_float=lambda x: round(float(x), 6))\n","with open('embeddings/susimcse/all.json', \"w\") as outfile:\n","    json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJvU-pLV0wqr"},"outputs":[],"source":["# susimcse queries\n","for i, ids in enumerate([q_background_ids, q_method_ids, q_result_ids]):\n","    d = dict()\n","    err_pids = list()\n","    s = ''\n","    if i == 0:\n","        s = 'background'\n","    elif i == 1:\n","        s = 'method'\n","    elif i == 2:\n","        s = 'result'\n","    for pid in ids:\n","        try:\n","            doc = facet_sentences(pid, s)\n","            if len(doc) == 0:\n","                d[pid] = list()\n","                continue\n","            susimcse_array = get_susimcse_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","            d[pid] = susimcse_array\n","        except:\n","            err_pids.append(pid)\n","    print(f'len(err_pids), {s} = {len(err_pids)}')\n","    print(err_pids)\n","    with open(f'embeddings/susimcse/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n","    d = json.load(open(f'embeddings/susimcse/{s}.json'),\n","                parse_float=lambda x: round(float(x), 6))\n","    with open(f'embeddings/susimcse/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9tffg-c0wqr","outputId":"529d76cf-2553-498c-f6a6-1f23286e19a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["768\n","len(err_pids) = 0\n","[]\n"]}],"source":["# specter all\n","d = dict()\n","err_pids = list()\n","i = 0\n","for pid in papers:\n","    # if i == 1:\n","    #     break\n","    # i += 1\n","    # if i % 50 == 0:\n","    #     print(i)\n","    try:\n","        doc = papers[pid].abstract\n","        if len(doc) == 0:\n","            d[pid] = list()\n","            continue\n","        specter_array = get_specter_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","        print(len(specter_array[0]))\n","        # print(specter_array)\n","        d[pid] = specter_array[0]\n","    except:\n","        err_pids.append(pid)\n","print(f'len(err_pids) = {len(err_pids)}')\n","print(err_pids)\n","with open(\"embeddings/specter/all.json\", \"w\") as outfile:\n","    json.dump(d, outfile)\n","d = json.load(open('embeddings/specter/all.json'),\n","              parse_float=lambda x: round(float(x), 6))\n","with open('embeddings/specter/all.json', \"w\") as outfile:\n","    json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYFnxFV_0wqr"},"outputs":[],"source":["# specter queries\n","for i, ids in enumerate([q_background_ids, q_method_ids, q_result_ids]):\n","    d = dict()\n","    err_pids = list()\n","    s = ''\n","    if i == 0:\n","        s = 'background'\n","    elif i == 1:\n","        s = 'method'\n","    elif i == 2:\n","        s = 'result'\n","    for pid in ids:\n","        try:\n","            doc = facet_sentences(pid, s)\n","            if len(doc) == 0:\n","                d[pid] = list()\n","                continue\n","            specter_array = get_specter_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","            d[pid] = specter_array[0]\n","        except:\n","            err_pids.append(pid)\n","    print(f'len(err_pids), {s} = {len(err_pids)}')\n","    print(err_pids)\n","    with open(f'embeddings/specter/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n","    d = json.load(open(f'embeddings/specter/{s}.json'),\n","                parse_float=lambda x: round(float(x), 6))\n","    with open(f'embeddings/specter/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYyKddL00wqs","outputId":"59645f9a-583c-4a05-9403-3a6e482b6c84"},"outputs":[{"name":"stdout","output_type":"stream","text":["768\n","len(err_pids) = 0\n","[]\n"]}],"source":["# scibert uncased all\n","d = dict()\n","err_pids = list()\n","i = 0\n","for pid in papers:\n","    # if i == 1:\n","    #     break\n","    i += 1\n","    if i % 50 == 0:\n","        print(i)\n","    try:\n","        doc = papers[pid].abstract\n","        if len(doc) == 0:\n","            d[pid] = list()\n","            continue\n","        scibert_uncased_array = get_scibert_uncased_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","        # print(len(scibert_uncased_array[0]))\n","        # print(scibert_uncased_array)\n","        d[pid] = scibert_uncased_array[0]\n","    except:\n","        err_pids.append(pid)\n","print(f'len(err_pids) = {len(err_pids)}')\n","print(err_pids)\n","with open(\"embeddings/scibert_uncased/all.json\", \"w\") as outfile:\n","    json.dump(d, outfile)\n","d = json.load(open('embeddings/scibert_uncased/all.json'),\n","              parse_float=lambda x: round(float(x), 6))\n","with open('embeddings/scibert_uncased/all.json', \"w\") as outfile:\n","    json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"au1hSRSM0wqs"},"outputs":[],"source":["# scibert_uncased queries\n","for i, ids in enumerate([q_background_ids, q_method_ids, q_result_ids]):\n","    d = dict()\n","    err_pids = list()\n","    s = ''\n","    if i == 0:\n","        s = 'background'\n","    elif i == 1:\n","        s = 'method'\n","    elif i == 2:\n","        s = 'result'\n","    for pid in ids:\n","        try:\n","            doc = facet_sentences(pid, s)\n","            if len(doc) == 0:\n","                d[pid] = list()\n","                continue\n","            scibert_uncased_array = get_scibert_uncased_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","            d[pid] = scibert_uncased_array[0]\n","        except:\n","            err_pids.append(pid)\n","    print(f'len(err_pids), {s} = {len(err_pids)}')\n","    print(err_pids)\n","    with open(f'embeddings/scibert_uncased/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n","    d = json.load(open(f'embeddings/scibert_uncased/{s}.json'),\n","                parse_float=lambda x: round(float(x), 6))\n","    with open(f'embeddings/scibert_uncased/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhyTRlmE0wqs","outputId":"ae93271f-8568-4481-d763-4a35d542c535"},"outputs":[{"name":"stdout","output_type":"stream","text":["768\n","len(err_pids) = 0\n","[]\n"]}],"source":["# scibert cased all\n","d = dict()\n","err_pids = list()\n","i = 0\n","for pid in papers:\n","    # if i == 1:\n","    #     break\n","    i += 1\n","    if i % 50 == 0:\n","        print(i)\n","    try:\n","        doc = papers[pid].abstract\n","        if len(doc) == 0:\n","            d[pid] = list()\n","            continue\n","        scibert_cased_array = get_scibert_cased_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","        # print(len(scibert_cased_array[0]))\n","        # print(scibert_cased_array)\n","        d[pid] = scibert_cased_array[0]\n","    except:\n","        err_pids.append(pid)\n","print(f'len(err_pids) = {len(err_pids)}')\n","print(err_pids)\n","with open(\"embeddings/scibert_cased/all.json\", \"w\") as outfile:\n","    json.dump(d, outfile)\n","d = json.load(open('embeddings/scibert_cased/all.json'),\n","              parse_float=lambda x: round(float(x), 6))\n","with open('embeddings/scibert_cased/all.json', \"w\") as outfile:\n","    json.dump(d, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aBwxeAt00wqs"},"outputs":[],"source":["# scibert_cased queries\n","for i, ids in enumerate([q_background_ids, q_method_ids, q_result_ids]):\n","    d = dict()\n","    err_pids = list()\n","    s = ''\n","    if i == 0:\n","        s = 'background'\n","    elif i == 1:\n","        s = 'method'\n","    elif i == 2:\n","        s = 'result'\n","    for pid in ids:\n","        try:\n","            doc = facet_sentences(pid, s)\n","            if len(doc) == 0:\n","                d[pid] = list()\n","                continue\n","            scibert_cased_array = get_scibert_cased_embedding(merge_list_to_str(doc)).detach().numpy().tolist()\n","            d[pid] = scibert_cased_array[0]\n","        except:\n","            err_pids.append(pid)\n","    print(f'len(err_pids), {s} = {len(err_pids)}')\n","    print(err_pids)\n","    with open(f'embeddings/scibert_cased/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)\n","    d = json.load(open(f'embeddings/scibert_cased/{s}.json'),\n","                parse_float=lambda x: round(float(x), 6))\n","    with open(f'embeddings/scibert_cased/{s}.json', \"w\") as outfile:\n","        json.dump(d, outfile)"]}],"metadata":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"orig_nbformat":4,"colab":{"name":"embeddings.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}